---
title: "Análise da performance dos classificadores"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(GGally)
library(dplyr)

datasets <- c('10797', '38959', '45827', '53752', '57297', '62944', '70947', '71053', '7904')
project_path <- "~/Documents/codelab.nosync/biomarker_id/"
results_path <- paste(project_path, "results/training/", sep = "")
data_path <- paste(project_path, "data/processed/train/", sep = "")
```

## Análise de correlação dos genes
Correlação entre os 5 genes mais bem ranqueados de acordo com os seguintes parâmetros:

- **50 bags**

- **Information gain** as feature selector

- **Mean** as aggregation method

Os genes mais bem ranqueados, de fato, parecem separar bem os dados entre as duas classes com exceção do GSE71053. A questão é: esse conjunto de genes é o mais representativo em relação aos dados?
```{r echo=FALSE, fig.height=7, fig.width=10}
accuracies <- list()
recalls <- list()

for(code in datasets) {
  svmRadial <- readRDS(paste(results_path, "GSE", code, "_common/models/svmRadial.rds", sep = ""))
  mean <- readRDS(paste(results_path, "GSE", code, "_common/final_rankings/mean.rds", sep = ""))
  load(paste(data_path, "GSE", code, "_common.rda", sep = ""))
  
  acc <- max(svmRadial[["mean"]][["infoGain"]][["50"]][["5"]][["metrics"]][["Accuracy"]])
  accuracies[[code]] <- acc
  
  recall <- max(svmRadial[["mean"]][["infoGain"]][["50"]][["5"]][["metrics"]][["Recall"]])
  recalls[[code]] <- recall
  
  ranking <- mean[["infoGain"]][["50"]]
  top5_features <- ranking[c(1:5), 1]
  filtered <- x[, top5_features]
  filtered["TARGET"] <- y
  
  p <- ggpairs(filtered, mapping = aes(color = TARGET), columns = 1:5, 
               title=paste("GSE", code, ", SVM, acc=", round(acc, 3), ", recall=", round(recall, 3), sep = ""),
               progress = FALSE)
  
  print(p)
}
```

## Análise da performance
Para responder a questão anterior, para cada dataset **5 genes** foram selecionados aleatoriamente e um modelo **SVM radial** foi treinado com os **dados de validação** (*dados originais + SMOTE*). Esse processo foi **repetido 100 vezes** para garantir a aleatoriedade dos genes. O resultado final é a **performance média** do modelo para cada dataset. Comparamos os resultados com a acurácia obtida pelos modelos treinados com os 5 genes selecionados utilizando **50 bags** e **information gain**.
```{r echo=FALSE}
# source(paste(project_path, "scripts/utils/train.R", sep = ""))
trainPorportion <- 0.6
results <- list()

for(code in datasets) {
  # filepath <- paste(data_path, "GSE", code, "_common.rda", sep = "")
  obj_path <- paste(project_path, "reports/gene_analysis/GSE", code, "_results.rds", sep = "")
  # source(paste(project_path, "scripts/utils/data.R", sep = ""))
  
  features <- names(x)
  
  obj <- readRDS(obj_path)
  acc <- obj[["acc"]]
  recall <- obj[["recall"]]

  # for(i in 1:100) {
  #   selected <- sample(features, 5)
  #   model <- trainModel(validationX[, selected], validationY, getLOOCV(), "svmRadial")
  #   acc <- c(acc, max(model[["results"]][["Accuracy"]]))
  #   recall <- c(recall, max(model[["results"]][["Sens"]]))
  # }
  
  # obj <- list(acc = acc, recall = recall)
  # saveRDS(obj, obj_path)
  
  results[[paste("GSE", code, sep = "")]] <- c(accuracies[[code]], mean(acc), recalls[[code]], mean(recall))
}

results <- data.frame(results, row.names = c("FS_ACC", "RANDOM_ACC", "FS_RECALL", "RANDOM_RECALL"))
results <- data.frame(t(results), row.names = names(results))
results <- results %>% mutate(DIFF_ACC = RANDOM_ACC - FS_ACC, DIFF_RECALL = RANDOM_RECALL - FS_RECALL, DATASET = rownames(results))

print(results %>% select(DATASET, FS_ACC, RANDOM_ACC, DIFF_ACC, FS_RECALL, RANDOM_RECALL, DIFF_RECALL))
```

Estranhamente, a performance dos modelos treinados com genes aleatórios é satisfatória. Isso quer dizer que qualquer conjunto de 5 genes é representativo dos dados? Algumas performances mantiveram-se altas, como em GSE10797, outras cairam um pouco (GSE71053). A seguir, vamos analisar as diferenças dos *information gain* scores entre os genes de um ranking.

## Análise dos rankings
Para cada dataset, carregamos o ranking final agrupado por média a partir de 50 rankings gerados por *information gain*. Calculamos a diferença de scores adjacentes no rankings da posição 1 até a posição 200. O gráfico abaixo apresenta essas diferenças como um  *heatmap*.

```{r echo=FALSE, fig.height=30, fig.width=20}
diff_heatmap <- function(fs) {
  differences <- data.frame(t(data.frame(row.names = c("DATASET", "INDEX", "DIFF"))))

  for(code in datasets) {
    mean <- readRDS(paste(results_path, "GSE", code, "_common/final_rankings/mean.rds", sep = ""))
    ranking <- mean[[fs]][["50"]]
    
    diff_score <- diff(ranking$score[1:200])
    n <- length(diff_score)
    
    dataset <- replicate(n, paste("GSE", code, sep = ""))
    index <- c(1:n)
    
    differences <- rbind(differences, list(DATASET = dataset, INDEX = index, DIFF = diff_score))
    differences$DATASET <- as.character(differences$DATASET)
  }
  
  differences <- differences[order(differences$INDEX), ]
  differences$DATASET <- as.factor(differences$DATASET)
  differences$INDEX <- as.factor(differences$INDEX)
  
  g <- ggplot(differences, aes(x = DATASET, y = INDEX, fill = DIFF)) +
       geom_tile() + 
       scale_fill_gradient(low="red", high="white") + 
       ggtitle(paste("FS =", fs))
  
  return(g)
}

print(diff_heatmap("infoGain"))
```

Os genes mais informativos -- nas primeiras posições dos rankings -- dos datasets GSE(10797, 57297, 71053, 7904) gerados pelo *information gain* e agregados por média não possuem diferença entre si. Isso significa que o information gain retornou o mesmo score para um grande conjunto de genes. Além disso, a diferença apresentada nos outros rankings não é significativa. A maior diferença identifica é **0.08**, o que explicaria o bom desempenho dos classificadores com genes aleatórios porque, sengundo o information gain, todos os genes são suficientemente informativos. Para investigar melhor os scores obtidos para cada gene, realizaremos a mesma análise para outros métodos de seleção de atributos.

```{r echo=FALSE, fig.height=30, fig.width=20}
print(diff_heatmap("mRMR"))
```
No caso do mRMR, podemos perceber que as maiores diferenças concentram-se nas primeiras posições dos rankings. Isso significa algumas coisas:

1. Pelo mRMR, há genes que distinguem-se pela representatividade nos dados principalmente em GSE62944 e GSE53752;

2. O bom desempenho dos modelos treinados com genes aleatórios não pode ser explicado pelo argumento mencionado anteriormente;

3. As diferenças nas primeiras posições dos rankings são maiores comparadas com os resultados do infoGain;

4. Por outro lado, as diferenças apresentadas pelo mRMR ainda são pequenas, o que pode corroborar com o bom desempenho mencionado no item 2.

```{r echo=FALSE, fig.height=30, fig.width=20}
print(diff_heatmap("anova"))
```

O método anova, assim como o mRMR, apresenta as maiores diferenças no topo do ranking. O que poderia indicar que, de fato, existem genes mais representativos dos dados do que outros. Próximos passos é entender melhor como cada método funciona, o que cada um leva em consideração para selecionar os genes. Isso vai nos ajudar a entender as diferenças nos resultados apresentados.



